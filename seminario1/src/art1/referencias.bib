@inproceedings{Fischer_2021,
  abstract        = {Pattern set mining has been successful in discovering small sets of highly informative and useful patterns from data. To find good models, existing methods heuristically explore the twice-exponential search space over all possible pattern sets in a combinatorial way, by which they are limited to data over at most hundreds of features, as well as likely to get stuck in local minima. Here, we propose a gradient based optimization approach that allows us to efficiently discover high-quality pattern sets from data of millions of rows and hundreds of thousands of features.In particular, we propose a novel type of neural autoencoder called BinaPs, using binary activations and binarizing weights in each forward pass, which are directly interpretable as conjunctive patterns. For training, optimizing a data-sparsity aware reconstruction loss, continuous versions of the weights are learned in small, noisy steps. This formulation provides a link between the discrete search space and continuous optimization, thus allowing for a gradient based strategy to discover sets of high-quality and noise-robust patterns. Through extensive experiments on both synthetic and real world data, we show that BinaPs discovers high quality and noise robust patterns, and unique among all competitors, easily scales to data of supermarket transactions or biological variant calls.},
  address         = {New York, NY, USA},
  author          = {Fischer, Jonas and Vreeken, Jilles},
  booktitle       = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  doi             = {10.1145/3447548.3467348},
  isbn            = {9781450383325},
  keywords        = {pattern mining, neural networks, explainability, autoencoder},
  location        = {Virtual Event, Singapore},
  numpages        = {10},
  pages           = {383â€“392},
  publisher       = {Association for Computing Machinery},
  series          = {KDD '21},
  title           = {Differentiable Pattern Set Mining},
  url             = {https://doi.org/10.1145/3447548.3467348},
  url-access-date = {2025-06-13},
  year            = {2021}
}